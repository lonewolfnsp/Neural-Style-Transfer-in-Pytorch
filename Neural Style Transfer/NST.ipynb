{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c48ae0ab26f1188fd450208d348e0b7ff8ba5b28"
   },
   "source": [
    "# Artistic Neural Style Transfer using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9a813e84ce0b0303d417586ac7e2debb084557e9",
    "colab_type": "text",
    "id": "KV55L4uUbP5I"
   },
   "source": [
    "In this kernel, weâ€™ll implement the style transfer method that is outlined in the paper, [Image Style Transfer Using Convolutional Neural Networks, by Gatys](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf) in PyTorch.\n",
    "\n",
    "In this paper, style transfer uses the features found in the 19-layer VGG Network, which is comprised of a series of convolutional and pooling layers, and a few fully-connected layers. In the image below, the convolutional layers are named by stack and their order in the stack. Conv_1_1 is the first convolutional layer that an image is passed through, in the first stack. Conv_2_1 is the first convolutional layer in the *second* stack. The deepest convolutional layer in the network is conv_5_4.\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/raw/master/style-transfer/notebook_ims/vgg19_convlayers.png\" width=80% />\n",
    "\n",
    "### Separating Style and Content\n",
    "\n",
    "Style transfer relies on separating the content and style of an image. Given one content image and one style image, we aim to create a new, _target_ image which should contain our desired content and style components:\n",
    "* objects and their arrangement are similar to that of the **content image**\n",
    "* style, colors, and textures are similar to that of the **style image**\n",
    "\n",
    "An example is shown below, where the content image is of a cat, and the style image is of [Hokusai's Great Wave](https://en.wikipedia.org/wiki/The_Great_Wave_off_Kanagawa). The generated target image still contains the cat but is stylized with the waves, blue and beige colors, and block print textures of the style image!\n",
    "\n",
    "<img src='https://github.com/udacity/deep-learning-v2-pytorch/raw/master/style-transfer/notebook_ims/style_tx_cat.png' width=80% />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7b1206e82683c9b6d0f269971def5655c3a1f7d8",
    "colab": {},
    "colab_type": "code",
    "id": "oVlHGJYMbP5N"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv \n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models \n",
    "import IPython.display as display\n",
    "from tqdm import tqdm \n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mean = np.asarray([ 0.485, 0.456, 0.406 ])\n",
    "std = np.asarray([ 0.229, 0.224, 0.225 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d879940d5cb2711463b02eb363ed70bf0a0e15ef",
    "colab_type": "text",
    "id": "7d0XIknPbP5Z"
   },
   "source": [
    "## Load in VGG19 (features)\n",
    "\n",
    "VGG19 is split into two portions:\n",
    "* `vgg19.features`, which are all the convolutional and pooling layers\n",
    "* `vgg19.classifier`, which are the three linear, classifier layers at the end\n",
    "\n",
    "We only need the `features` portion, which we're going to load in and \"freeze\" the weights of, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f7710325ec38cfb299bfe8307ed5d87beaeedecc",
    "colab": {},
    "colab_type": "code",
    "id": "jVr5_jzvbP5c"
   },
   "outputs": [],
   "source": [
    "# get the \"features\" portion of VGG19 (we will not need the \"classifier\" portion)\n",
    "model = models.vgg19(pretrained=True).features\n",
    "# without freezing the weights for each layer, training will be very slow. \n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "56714eecb4da53210dffc55d4aa1f7217512846d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 726
    },
    "colab_type": "code",
    "id": "jfKwUYt0bP6F",
    "outputId": "84b031fd-5406-4ec9-d766-9c73803cf7bd"
   },
   "outputs": [],
   "source": [
    "# move the model to GPU, if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2add82f05bc066531e1cc3d7af8015bdbf006867",
    "colab_type": "text",
    "id": "d4VsYd9bbP6n"
   },
   "source": [
    "### Load in Content and Style Images\n",
    "\n",
    "You can load in any images you want! Below, we've provided a helper function for loading in any type and size of image. The `load_image` function also converts images to normalized Tensors.\n",
    "\n",
    "Additionally, it will be easier to have smaller images and to squash the content and style images so that they are of the same size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "55d17c46c21cc9ff0dbd4cfdcd53abda7aa08cd5",
    "colab_type": "text",
    "id": "DXzKhDvpbP7T"
   },
   "source": [
    "Next, I'm loading in images by file name and forcing the style image to be the same size as the content image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_as_tensor(img_path, max_size=1024, shape=None):        \n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    w, h=image.size[0], image.size[1]\n",
    "    # print(f'size={image.size}, h={h}, w={w} ')    \n",
    "    \n",
    "    # large images will slow down processing    \n",
    "    if max(image.size) > max_size:\n",
    "        scale=max_size/max(image.size)\n",
    "        size = torch.Size((int(h*scale), int (w*scale)))\n",
    "    else:        \n",
    "            size=torch.Size((h, w))\n",
    "            \n",
    "    if shape is not None:\n",
    "        size=shape\n",
    "             \n",
    "    tfm = transforms.Compose([\n",
    "        transforms.Resize(size), \n",
    "        transforms.ToTensor(),\n",
    "        # normalize image based on mean and std of ImageNet dataset\n",
    "        transforms.Normalize(mean, std, inplace=True), \n",
    "        transforms.Lambda(lambda x: x.mul(255.))\n",
    "        ])\n",
    "\n",
    "    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
    "    return tfm(image)[:3,:,:].unsqueeze(0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the content, style images and other settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content='louvre2.jpg'\n",
    "style='style2.jpg'\n",
    "useLayers='relu' # conv or relu layers in vgg\n",
    "optimizer='adam' # adam or lbfgs\n",
    "stylized_filename=f'{optimizer} {useLayers} {style[:-4] } stylized {content[:-4]}.jpg'\n",
    "print(stylized_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d5505f533cf4944c0d316fe9bf982946a4a14b37",
    "colab": {},
    "colab_type": "code",
    "id": "AtrlePgEbP7W"
   },
   "outputs": [],
   "source": [
    "# load in content and style image\n",
    "content = load_image_as_tensor(content).to(device)\n",
    "print(f'content shape={content.shape} ')\n",
    "# Resize style to match content, makes code easier\n",
    "style = load_image_as_tensor (style, shape=content.shape[-2:]).to(device)\n",
    "print(f'style shape={style.shape} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showTensorImage(img, filename=None):    \n",
    "    tfm=transforms.Compose([\n",
    "        # reverse the normalization\n",
    "        transforms.Lambda(lambda x: x.div(255.) ), \n",
    "        transforms.Normalize((-1 * mean / std), (1.0 / std),inplace=True) \n",
    "        ])\n",
    "    img2=tfm(img.cpu().squeeze(0))\n",
    "    if filename is not None:  \n",
    "        tv.utils.save_image(img2, filename)\n",
    "    return img2.permute(1, 2, 0).detach().numpy()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20) )\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(showTensorImage(content))\n",
    "plt.title('content image')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2, 2)\n",
    "plt.imshow(showTensorImage(style) ) \n",
    "plt.title('style image')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2717bff1f683f8d92f498cececf997dac2b3cf2c",
    "colab_type": "text",
    "id": "_DkY4OvmbP8g"
   },
   "source": [
    "## Content and Style Features\n",
    "\n",
    "Below, complete the mapping of layer names to the names found in the paper for the _content representation_ and the _style representation_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b46a9f4abf05d89bd7c60fe7951525bffd1640d",
    "colab": {},
    "colab_type": "code",
    "id": "FV5THOfzbP8j"
   },
   "outputs": [],
   "source": [
    "def getModelOutputs(image, layers):                    \n",
    "    outputs={}                \n",
    "    x = image\n",
    "    cnt=len(layers)\n",
    "    # model._modules is a dictionary holding each module in the model            \n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)        \n",
    "        if name in layers:\n",
    "            outputs[name]=x \n",
    "            cnt-=1\n",
    "            if cnt==0: # outputs from required layers obtained, break out\n",
    "                break               \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unrollTensor(A):\n",
    "    if len(A.shape)==4:\n",
    "        A=A.squeeze(0)\n",
    "    c, h, w=A.shape\n",
    "    return A.reshape(c, -1), c*h*w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "388ac52cd09142a4a4219da4a361215cdbc85499",
    "colab_type": "text",
    "id": "gNKjuZZBbP8r"
   },
   "source": [
    "## Content Cost Function $J_{content}(C,G)$\n",
    "\n",
    "One goal you should aim for when performing NST is for the content in generated image G to match the content of image C. A method to achieve this is to calculate the content cost function, which will be defined as:\n",
    "\n",
    "$$J_{content}(C,G) =  \\frac{1}{4 \\times n_H \\times n_W \\times n_C}\\sum _{ \\text{all entries}} (a^{(C)} - a^{(G)})^2\\tag{1} $$\n",
    "\n",
    "* Here, $n_H, n_W$ and $n_C$ are the height, width and number of channels of the hidden layer you have chosen, and appear in a normalization term in the cost. \n",
    "* For clarity, note that $a^{(C)}$ and $a^{(G)}$ are the 3D volumes corresponding to a hidden layer's activations. \n",
    "* In order to compute the cost $J_{content}(C,G)$, it might also be convenient to unroll these 3D volumes into a 2D matrix, as shown below.\n",
    "* Technically this unrolling step isn't needed to compute $J_{content}$, but it will be good practice for when you do need to carry out a similar operation later for computing the style cost $J_{style}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContentCost(a_C_unrolled, a_G):        \n",
    "    #unroll the tensors to be (b, c, h x w)    \n",
    "    a_G_unrolled,chw =unrollTensor(a_G)            \n",
    "    return torch.sum((a_C_unrolled - a_G_unrolled)**2) / (4*chw)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRAM Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "65e77ee27728fd35e8e1df9900b0623aa769a50e",
    "colab": {},
    "colab_type": "code",
    "id": "OQykPzj9bP8u"
   },
   "outputs": [],
   "source": [
    "def getGRAMmatrix(A):    \n",
    "    return torch.mm(A, A.t())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Cost\n",
    "\n",
    "$$J_{style}^{[l]}(S,G) = \\frac{1}{(2 \\times n_C \\times n_H \\times n_W)^2} \\sum _{i=1}^{n_C}\\sum_{j=1}^{n_C}(G^{(S)}_{(gram)i,j} - G^{(G)}_{(gram)i,j})^2\\tag{2} $$\n",
    "\n",
    "* $G_{gram}^{(S)}$ Gram matrix of the \"style\" image.\n",
    "* $G_{gram}^{(G)}$ Gram matrix of the \"generated\" image.\n",
    "* Make sure you remember that this cost is computed using the hidden layer activations for a particular hidden layer in the network $a^{[l]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStyCost(styleGRAMs, target_outputs, style_weights):\n",
    "    stylecost=0.\n",
    "    for k, v in style_weights.items():\n",
    "        a_G_unrolled, chw = unrollTensor(target_outputs[k])    \n",
    "        G_G=getGRAMmatrix(a_G_unrolled)                           \n",
    "        cost=torch.sum((styleGRAMs[k] - G_G)**2) / ((2*chw)**2)     \n",
    "        stylecost+= v * cost\n",
    "    return stylecost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Variation Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTotalVariationCost(y):\n",
    "    return torch.sum(torch.abs(y[:, :, :, :-1] - y[:, :, :, 1:])) + \\\n",
    "           torch.sum(torch.abs(y[:, :, :-1, :] - y[:, :, 1:, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Cost\n",
    "\n",
    "$$J(G) = \\alpha J_{content}(C,G) + \\beta J_{style}(S,G)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTotalCost(J_content, J_style,tvcost, alpha = 2, beta = 40):\n",
    "    total=(J_content * alpha) + (beta*J_style) + tvcost\n",
    "    return total "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the generated image with random noise and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise=torch.randn(content.shape).to(device)\n",
    "target =torch.autograd.Variable(content.clone()+noise, requires_grad=True)\n",
    "print(f'target has shape: {target.shape} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f4282cf943d7f7d2cf10918baed021793104332b",
    "colab_type": "text",
    "id": "ui41lmB4bP9o"
   },
   "source": [
    "## Weights for each style layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a3867b0dd38b95cb706fb0e416f4c9d7b7433f00",
    "colab": {},
    "colab_type": "code",
    "id": "T6xsdRMlbP9r"
   },
   "outputs": [],
   "source": [
    "style_weights={}\n",
    "content_layer=''\n",
    "\n",
    "if useLayers=='conv':\n",
    "    content_layer='21'\n",
    "    style_weights = {'0':0.55, '5':0.65, '10':.75, '19':0.85, '28':0.95}\n",
    "elif useLayers=='relu':\n",
    "    content_layer='22'\n",
    "    style_weights = {'1':0.55, '6':0.65, '11':0.75, '20':0.85, '29':0.95}\n",
    "featureLayers=set(style_weights.keys())\n",
    "featureLayers.add(content_layer)\n",
    "print(f'featureLayers has {len(featureLayers)} layers: {featureLayers} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c19ab2694abc56e1388c464c8e917f0ad47f7e01",
    "colab_type": "text",
    "id": "9dCQvrwAbP91"
   },
   "source": [
    "## Get content and Style features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before training, we obtain the necessary features for the content and style images. As these features are not changed during training, but merely used for computing losses for the output image, doing so will speed up training drastically as we don't have to keep computing them with every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_outputs = getModelOutputs(content, [content_layer])\n",
    "a_C=content_outputs[content_layer]\n",
    "a_Cunrolled, _ =unrollTensor(a_C)\n",
    "\n",
    "style_outputs = getModelOutputs(style, featureLayers)    \n",
    "styleGRAMs={}\n",
    "for k, v in style_outputs.items():    \n",
    "    if k in style_weights.keys(): # skip the content layer\n",
    "        unrolled, _ =unrollTensor(v)\n",
    "        styleGRAMs[k] = getGRAMmatrix(unrolled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingCosts(target_img):\n",
    "    \n",
    "    target_outputs = getModelOutputs(target_img, featureLayers)\n",
    "    \n",
    "    contentCost= getContentCost(a_Cunrolled, target_outputs[content_layer])            \n",
    "    styleCost = getStyCost(styleGRAMs, target_outputs, style_weights)           \n",
    "    tvCost=getTotalVariationCost(target_img)\n",
    "    \n",
    "    totalCost =getTotalCost(contentCost, styleCost, tvCost)\n",
    "\n",
    "    return totalCost, contentCost, styleCost, tvCost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9bcc49bb9590c4a671eecaed7f359be604aa29f8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1222
    },
    "colab_type": "code",
    "id": "fGgRrXjLbP94",
    "outputId": "69e4b4d0-bff6-4884-bc4d-f76340b7565c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trainAdam(totallosses, contentlosses, stylelosses,tvlosses,  epochs=3000, lr=1.25, show_step=500):\n",
    "    opt = optim.Adam([target], lr=lr)    \n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(opt,max_lr=lr*5., epochs=epochs,steps_per_epoch=1)\n",
    "\n",
    "    for ep in tqdm(range(epochs)):        \n",
    "        opt.zero_grad()\n",
    "        Tloss, CLoss, SLoss, TVLoss =getTrainingCosts(target)           \n",
    "\n",
    "        totallosses +=[Tloss.item() ]\n",
    "        stylelosses +=[SLoss.item()]\n",
    "        contentlosses +=[CLoss.item()]\n",
    "        tvlosses += [TVLoss.item()]\n",
    "        \n",
    "        # update your target image    \n",
    "        Tloss.backward()\n",
    "        opt.step()\n",
    "        sched.step()    \n",
    "        \n",
    "        step=ep+1\n",
    "        if  step % show_step == 0 or step==1:        \n",
    "            display.clear_output(wait=True)        \n",
    "            print(f'epoch {step}/{epochs}: loss={total_losses[-1]}')                \n",
    "            if step < epochs:\n",
    "                plt.imshow(showTensorImage(target))\n",
    "            else: \n",
    "                plt.imshow(showTensorImage(target, stylized_filename))\n",
    "            plt.axis('off')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with LBFGS optimizer\n",
    "### L-BFGS belongs to a class of optimizers call quasi newton optimizers. There's no need to specify learning rates for training, it will determine the best step size and adjust constantly during training. Only drawback is it has no mini batch support so it has to take the entire batch. Given the HUGE step size of NST, the 3 images required for training, this makes it very suitable for this scenario. Note the strange way of calling it, as an inner function, and it will call itself repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLBFGS(totallosses, contentlosses, stylelosses,tvlosses, epochs=1000):\n",
    "    opt = optim.LBFGS((target,), max_iter=epochs, line_search_fn='strong_wolfe')\n",
    "    step=1\n",
    "    pbar=tqdm(total=epochs)\n",
    "\n",
    "    def closure():\n",
    "        nonlocal step\n",
    "        nonlocal totallosses\n",
    "        nonlocal contentlosses\n",
    "        nonlocal stylelosses \n",
    "        nonlocal tvlosses\n",
    "        \n",
    "        if torch.is_grad_enabled():\n",
    "            opt.zero_grad()        \n",
    "            \n",
    "        Tloss, CLoss, SLoss, tvloss =getTrainingCosts(target)           \n",
    "\n",
    "        totallosses +=[Tloss.item()]\n",
    "        stylelosses +=[SLoss.item()]\n",
    "        contentlosses +=[CLoss.item()]\n",
    "        tvlosses += [tvloss.item()]\n",
    "\n",
    "        if Tloss.requires_grad:\n",
    "            Tloss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if step==epochs:\n",
    "                display.clear_output(wait=True)                        \n",
    "                plt.imshow(showTensorImage(target, stylized_filename))\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "        step+=1\n",
    "        pbar.update(1)\n",
    "        return Tloss\n",
    "                     \n",
    "    opt.step(closure)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_losses=[]\n",
    "contentLosses=[]\n",
    "styleLosses=[]\n",
    "tvLosses=[]\n",
    "\n",
    "if optimizer=='adam':\n",
    "    trainAdam(total_losses, contentLosses, styleLosses, tvLosses)\n",
    "elif optimizer=='lbfgs':\n",
    "    trainLBFGS(total_losses, contentLosses, styleLosses,tvLosses)\n",
    "else:\n",
    "    print('unknown optimizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3577b759c2054d0d312c004bcd2f46bfec3db14d",
    "colab_type": "text",
    "id": "bbZVdQq_bP-1"
   },
   "source": [
    "## Display the Target Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20) )\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(showTensorImage(content))\n",
    "plt.title('content')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(showTensorImage(style))\n",
    "plt.title('style image')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(showTensorImage(target))\n",
    "plt.title('stylized image')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15) )\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(total_losses)\n",
    "plt.title(\"total losses\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(contentLosses)\n",
    "plt.title(\"content losses\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(styleLosses)\n",
    "plt.title(\"style losses\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(tvLosses)\n",
    "plt.title(\"Total Variation losses\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- A very good pytorch neural style transfer [git repo](https://github.com/gordicaleksa/pytorch-neural-style-transfer)\n",
    "- Andrew Ng's video lecture on [neural style transfer](https://www.youtube.com/watch?v=ChoV5h7tw5A)\n",
    "- A good video on [pytorch neural style transfer](https://www.youtube.com/watch?v=imX4kSKDY7s&t=214s)\n",
    "- Good article on BFGS: [A very gentle introduction to BFGS optimizing algorithm](https://machinelearningmastery.com/bfgs-optimization-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Style_Transfer_Solution.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "interpreter": {
   "hash": "dd5645910c9f12221763b2aa836bb1f3cb241427948cb3afce762a164a0fd337"
  },
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
